{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainbow Memory: Make a dataset of each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should read json file which follows below format. \n",
    "\n",
    "[{\"klass\": \"truck\", \"file_name\": \"test/class1.txt\", \"idx\":6}, ...]\n",
    "\n",
    "You should change the file name as below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(r'C:\\Users\\cannedfish\\Desktop\\测试数据_(非密）面向未知机载AESA雷达的开集智能识别算法\\train.json')\n",
    "test = pd.read_json(r'C:\\Users\\cannedfish\\Desktop\\测试数据_(非密）面向未知机载AESA雷达的开集智能识别算法\\test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variables `rnd_seed` and `num_tasks` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed = 3 # random seed \n",
    "num_tasks = 4 # the number of tasks. \n",
    "np.random.seed(rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "klass = train.klass.unique()\n",
    "num_cls_per_task = len(klass) // num_tasks\n",
    "np.random.shuffle(klass)\n",
    "\n",
    "class2label = {cls_:idx for idx, cls_ in enumerate(klass)} \n",
    "train[\"label\"] = train.klass.apply(lambda x: class2label[x])\n",
    "test[\"label\"] = test.klass.apply(lambda x: class2label[x])\n",
    "\n",
    "task_class = np.split(klass, num_tasks)\n",
    "task_train = [train[train.klass.isin(tc)] for tc in task_class]\n",
    "task_test = [test[test.klass.isin(tc)] for tc in task_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disjoint Task Boundaries Benchmark\n",
    "\n",
    "Configure disjoint dataset which does not share the classes of each task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\collections\\disjoint\\pdw_disjoint_rand3_cls1_task0.json\n",
      "..\\collections\\disjoint\\pdw_disjoint_rand3_cls1_task1.json\n",
      "..\\collections\\disjoint\\pdw_disjoint_rand3_cls1_task2.json\n",
      "..\\collections\\disjoint\\pdw_disjoint_rand3_cls1_task3.json\n"
     ]
    }
   ],
   "source": [
    "origin_name = \"pdw\" # Need to change the name of your dataset.\n",
    "root = Path('../collections/disjoint')\n",
    "root.mkdir(exist_ok=True)\n",
    "\n",
    "for idx, train_task in enumerate(task_train):\n",
    "    file_name = origin_name\n",
    "    train_prefix = {'_disjoint':'', \n",
    "              '_rand':rnd_seed, \n",
    "              '_cls':num_cls_per_task,\n",
    "              '_task':idx\n",
    "             }\n",
    " \n",
    "    for name, value in train_prefix.items():\n",
    "        file_name += name + str(value)\n",
    "    file_path = (root/file_name).with_suffix('.json')\n",
    "    train_task.to_json(file_path, orient='records')\n",
    "    print(f\"{file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blurry Task Boundaries Benchmark\n",
    "\n",
    "Configure blurry task dataset which can share the classes of each task\n",
    "\n",
    "There are two types of classes per each task as described in the paper. \n",
    "\n",
    "- **Major Classes** account for 90(70) percent of whole dataset of the corresponding dataset in blurry-10(30). \n",
    "- **Minor Classes** account for 10(30) percent of whole dataset of the corresponding dataset in blurry-10(30). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_ratio = 0.9 # 0.9 for blurry-10, 0.7 for blurry-30.\n",
    "\n",
    "task_trainM = []\n",
    "task_trainN = []\n",
    "for t in task_train:\n",
    "    sub_task_trainN = []\n",
    "\n",
    "    taskM = t.sample(n = int(len(t) * 0.9), replace=False)\n",
    "    taskN = pd.concat([taskM, t]).drop_duplicates(keep=False)\n",
    "    taskN_size = len(taskN)\n",
    "\n",
    "    task_trainM.append(taskM)\n",
    "    for _ in range(len(task_train)-1):\n",
    "        sub_task_trainN.append(taskN.sample(n=taskN_size//(len(task_train)-1)))\n",
    "        \n",
    "    task_trainN.append(sub_task_trainN)\n",
    "    \n",
    "task_mixed_train = []\n",
    "for idx, task in enumerate(task_trainM):\n",
    "    other_task_samples = pd.DataFrame() \n",
    "    for j in range(len(task_trainM)):\n",
    "        if idx != j: \n",
    "            other_task_samples = pd.concat([other_task_samples, task_trainN[j].pop(0)])\n",
    "    mixed_task = pd.concat([task, other_task_samples])\n",
    "    task_mixed_train.append(mixed_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\collections\\blurry\\pdw_blurry10_rand3_cls1_task0.json\n",
      "..\\collections\\blurry\\pdw_blurry10_rand3_cls1_task1.json\n",
      "..\\collections\\blurry\\pdw_blurry10_rand3_cls1_task2.json\n",
      "..\\collections\\blurry\\pdw_blurry10_rand3_cls1_task3.json\n"
     ]
    }
   ],
   "source": [
    "origin_name = \"pdw\" # Need to change the name of your dataset.\n",
    "root = Path('../collections/blurry')\n",
    "root.mkdir(exist_ok=True)\n",
    "\n",
    "for idx, task in enumerate(task_mixed_train):\n",
    "    file_name = origin_name\n",
    "    prefix = {'_blurry':f'{int(round((1.0 - major_ratio)*100))}', \n",
    "              '_rand':rnd_seed, \n",
    "              '_cls':num_cls_per_task,\n",
    "              '_task':idx\n",
    "             }\n",
    "    \n",
    "    for name, value in prefix.items():\n",
    "        file_name += name + str(value)\n",
    "\n",
    "    file_path = (root/file_name).with_suffix('.json')\n",
    "    task.to_json(file_path, orient='records')\n",
    "    print(f\"{file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\collections\\test\\pdw_rand3_cls1_task0.json\n",
      "..\\collections\\test\\pdw_rand3_cls1_task1.json\n",
      "..\\collections\\test\\pdw_rand3_cls1_task2.json\n",
      "..\\collections\\test\\pdw_rand3_cls1_task3.json\n"
     ]
    }
   ],
   "source": [
    "origin_name = \"pdw\" # Need to change the name of your dataset.\n",
    "task_test = [test[test.klass.isin(tc)] for tc in task_class]\n",
    "\n",
    "root = Path('../collections/test')\n",
    "root.mkdir(exist_ok=True)\n",
    "\n",
    "for idx, task in enumerate(task_test):\n",
    "    file_name = origin_name\n",
    "    prefix = {'_rand':rnd_seed, \n",
    "              '_cls':num_cls_per_task,\n",
    "              '_task':idx\n",
    "             }\n",
    "    for name, value in prefix.items():\n",
    "        file_name += name + str(value)\n",
    "        \n",
    "    file_path = (root/file_name).with_suffix('.json')\n",
    "    task.to_json(file_path, orient='records')\n",
    "    print(f\"{file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
